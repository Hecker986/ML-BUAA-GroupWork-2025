{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c9ec8d",
   "metadata": {},
   "source": [
    "# è™šå‡æ–°é—»äºŒåˆ†ç±»ï¼ˆæ–‡æœ¬åŸºçº¿ + å›¾åƒèåˆ + é¢„è®­ç»ƒå¾®è°ƒï¼‰\n",
    "\n",
    "åŒ…å«ï¼š\n",
    "- æ–‡æœ¬åŸºçº¿ TF-IDF+LRï¼ˆå«è¿›åº¦æ‰“å°ä¸F1é˜ˆå€¼æœç´¢ï¼‰\n",
    "- å›¾åƒHSVç‰¹å¾ä¸æ–‡æœ¬ç¨€ç–æ‹¼æ¥çš„ late fusionï¼ˆå«è¿›åº¦æ‰“å°ï¼‰\n",
    "- é¢„è®­ç»ƒæ–‡æœ¬å¾®è°ƒï¼ˆRoBERTa/MacBERT å•æŠ˜ç¤ºä¾‹ï¼Œå«è¿›åº¦æ‰“å°ï¼‰\n",
    "- è§†è§‰ä¸»å¹²ï¼ˆResNet50ï¼‰ä¸ OpenCLIP ç‰¹å¾æå–èåˆï¼ˆCPU ç¯å¢ƒç¤ºä¾‹ï¼Œå«è¿›åº¦æ‰“å°ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdd10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ PART 0: ç¯å¢ƒå‡†å¤‡ ============================\n",
    "import os, random, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['http_proxy'] = ''\n",
    "os.environ['https_proxy'] = ''\n",
    "os.environ['HTTP_PROXY'] = ''\n",
    "os.environ['HTTPS_PROXY'] = ''\n",
    "\n",
    "CACHE = \".cache\"\n",
    "os.makedirs(CACHE, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79ad5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š æ•°æ®åŠ è½½ä¸­...\n",
      "============================================================\n",
      "âœ… è®­ç»ƒé›†: 7000 æ¡æ ·æœ¬, 13 ä¸ªç‰¹å¾\n",
      "âœ… æµ‹è¯•é›†: 3000 æ¡æ ·æœ¬, 11 ä¸ªç‰¹å¾\n",
      "ğŸ“‹ è®­ç»ƒé›†åˆ—å: ['text', 'image_path', 'entity_id', 'topic', 'fine-grained label', 'label', 'knowledge_embedding', 'description', 'relation', 'platform', 'author', 'date', 'comment']\n",
      "ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:\n",
      "   æ ‡ç­¾ 0: 4136 æ¡ (59.09%)\n",
      "   æ ‡ç­¾ 1: 2864 æ¡ (40.91%)\n",
      "ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ—: 'text'\n",
      "\n",
      "ğŸ”„ æ–‡æœ¬é¢„å¤„ç†ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†æ–‡æœ¬é¢„å¤„ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7000/7000 [00:00<00:00, 471572.89it/s]\n",
      "æµ‹è¯•é›†æ–‡æœ¬é¢„å¤„ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00<00:00, 453748.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\n",
      "   è®­ç»ƒé›† - å¹³å‡: 924.8, ä¸­ä½æ•°: 121.0, æœ€å¤§: 6001, æœ€å°: 3\n",
      "   æµ‹è¯•é›† - å¹³å‡: 975.0, ä¸­ä½æ•°: 122.0, æœ€å¤§: 6001, æœ€å°: 6\n",
      "\n",
      "âœ… æ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 1: æ•°æ®åŠ è½½ + æ–‡æœ¬é¢„å¤„ç† ============================\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š æ•°æ®åŠ è½½ä¸­...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_df = pd.read_pickle(\"train.pkl\")\n",
    "test_df  = pd.read_pickle(\"test_no_label.pkl\")\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒé›†: {train_df.shape[0]} æ¡æ ·æœ¬, {train_df.shape[1]} ä¸ªç‰¹å¾\")\n",
    "print(f\"âœ… æµ‹è¯•é›†: {test_df.shape[0]} æ¡æ ·æœ¬, {test_df.shape[1]} ä¸ªç‰¹å¾\")\n",
    "print(f\"ğŸ“‹ è®­ç»ƒé›†åˆ—å: {list(train_df.columns)}\")\n",
    "\n",
    "# æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "print(f\"ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f\"   æ ‡ç­¾ {label}: {count} æ¡ ({pct:.2f}%)\")\n",
    "\n",
    "text_col = \"news body text\" if \"news body text\" in train_df.columns else \"text\"\n",
    "print(f\"ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_col}'\")\n",
    "\n",
    "def normalize_text(s, max_len=6000):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n",
    "    if len(s) > max_len:\n",
    "        return s[:max_len//2] + \" \" + s[-max_len//2:]\n",
    "    return s\n",
    "\n",
    "print(\"\\nğŸ”„ æ–‡æœ¬é¢„å¤„ç†ä¸­...\")\n",
    "train_df[\"text\"] = [normalize_text(t) for t in tqdm(train_df[text_col], desc=\"è®­ç»ƒé›†æ–‡æœ¬é¢„å¤„ç†\", total=len(train_df))]\n",
    "test_df[\"text\"]  = [normalize_text(t) for t in tqdm(test_df[text_col], desc=\"æµ‹è¯•é›†æ–‡æœ¬é¢„å¤„ç†\", total=len(test_df))]\n",
    "\n",
    "# æ–‡æœ¬é•¿åº¦ç»Ÿè®¡\n",
    "train_lens = [len(t) for t in train_df[\"text\"]]\n",
    "test_lens = [len(t) for t in test_df[\"text\"]]\n",
    "print(f\"\\nğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\")\n",
    "print(f\"   è®­ç»ƒé›† - å¹³å‡: {np.mean(train_lens):.1f}, ä¸­ä½æ•°: {np.median(train_lens):.1f}, æœ€å¤§: {np.max(train_lens)}, æœ€å°: {np.min(train_lens)}\")\n",
    "print(f\"   æµ‹è¯•é›† - å¹³å‡: {np.mean(test_lens):.1f}, ä¸­ä½æ•°: {np.median(test_lens):.1f}, æœ€å¤§: {np.max(test_lens)}, æœ€å°: {np.min(test_lens)}\")\n",
    "\n",
    "X = train_df[\"text\"].values\n",
    "y = train_df[\"label\"].astype(int).values\n",
    "X_test = test_df[\"text\"].values\n",
    "\n",
    "print(\"\\nâœ… æ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e948a6",
   "metadata": {},
   "source": [
    "## TF-IDF + LogisticRegression 5æŠ˜åŸºçº¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a70de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹ TF-IDF + LogisticRegression 5æŠ˜äº¤å‰éªŒè¯\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 1/5 å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "   è®­ç»ƒé›†å¤§å°: 5600 (80.0%)\n",
      "   éªŒè¯é›†å¤§å°: 1400 (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3309 2291]\n",
      "   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\n",
      "   âœ… ç‰¹å¾ç»´åº¦: 200000\n",
      "   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\n",
      "   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯:  20%|â–ˆâ–ˆ        | 1/5 [00:22<01:29, 22.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â±ï¸  è€—æ—¶: 22.27ç§’\n",
      "   ğŸ“Š Fold 1 éªŒè¯é›†æŒ‡æ ‡:\n",
      "      F1: 0.68798 | å‡†ç¡®ç‡: 0.77000 | ç²¾ç¡®ç‡: 0.77342 | å¬å›ç‡: 0.61955\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 2/5 å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "   è®­ç»ƒé›†å¤§å°: 5600 (80.0%)\n",
      "   éªŒè¯é›†å¤§å°: 1400 (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3309 2291]\n",
      "   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\n",
      "   âœ… ç‰¹å¾ç»´åº¦: 200000\n",
      "   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\n",
      "   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:41<01:02, 20.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â±ï¸  è€—æ—¶: 19.63ç§’\n",
      "   ğŸ“Š Fold 2 éªŒè¯é›†æŒ‡æ ‡:\n",
      "      F1: 0.73066 | å‡†ç¡®ç‡: 0.79357 | ç²¾ç¡®ç‡: 0.78400 | å¬å›ç‡: 0.68412\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 3/5 å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "   è®­ç»ƒé›†å¤§å°: 5600 (80.0%)\n",
      "   éªŒè¯é›†å¤§å°: 1400 (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3309 2291]\n",
      "   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\n",
      "   âœ… ç‰¹å¾ç»´åº¦: 200000\n",
      "   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\n",
      "   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:05<00:43, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â±ï¸  è€—æ—¶: 23.26ç§’\n",
      "   ğŸ“Š Fold 3 éªŒè¯é›†æŒ‡æ ‡:\n",
      "      F1: 0.69549 | å‡†ç¡®ç‡: 0.76857 | ç²¾ç¡®ç‡: 0.75356 | å¬å›ç‡: 0.64572\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 4/5 å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "   è®­ç»ƒé›†å¤§å°: 5600 (80.0%)\n",
      "   éªŒè¯é›†å¤§å°: 1400 (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3309 2291]\n",
      "   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\n",
      "   âœ… ç‰¹å¾ç»´åº¦: 200000\n",
      "   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\n",
      "   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [01:28<00:22, 22.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â±ï¸  è€—æ—¶: 23.57ç§’\n",
      "   ğŸ“Š Fold 4 éªŒè¯é›†æŒ‡æ ‡:\n",
      "      F1: 0.73529 | å‡†ç¡®ç‡: 0.79429 | ç²¾ç¡®ç‡: 0.77670 | å¬å›ç‡: 0.69808\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 5/5 å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "   è®­ç»ƒé›†å¤§å°: 5600 (80.0%)\n",
      "   éªŒè¯é›†å¤§å°: 1400 (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3308 2292]\n",
      "   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\n",
      "   âœ… ç‰¹å¾ç»´åº¦: 200000\n",
      "   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\n",
      "   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5æŠ˜äº¤å‰éªŒè¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:52<00:00, 22.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â±ï¸  è€—æ—¶: 23.76ç§’\n",
      "   ğŸ“Š Fold 5 éªŒè¯é›†æŒ‡æ ‡:\n",
      "      F1: 0.70611 | å‡†ç¡®ç‡: 0.78000 | ç²¾ç¡®ç‡: 0.77731 | å¬å›ç‡: 0.64685\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ 5æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼Œè®¡ç®—æœ€ä½³é˜ˆå€¼...\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ æœ€ä½³é˜ˆå€¼: 0.4202\n",
      "ğŸ¯ OOF F1 Score: 0.72810\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨æœ€ä½³é˜ˆå€¼åçš„æ•´ä½“æŒ‡æ ‡:\n",
      "   F1: 0.72810 | å‡†ç¡®ç‡: 0.76100 | ç²¾ç¡®ç‡: 0.68106 | å¬å›ç‡: 0.78212\n",
      "\n",
      "ğŸ“Š å„æŠ˜è¯¦ç»†æŒ‡æ ‡:\n",
      "   Fold 1: F1=0.68798, Acc=0.77000, Prec=0.77342, Rec=0.61955\n",
      "   Fold 2: F1=0.73066, Acc=0.79357, Prec=0.78400, Rec=0.68412\n",
      "   Fold 3: F1=0.69549, Acc=0.76857, Prec=0.75356, Rec=0.64572\n",
      "   Fold 4: F1=0.73529, Acc=0.79429, Prec=0.77670, Rec=0.69808\n",
      "   Fold 5: F1=0.70611, Acc=0.78000, Prec=0.77731, Rec=0.64685\n",
      "\n",
      "âœ… TF-IDF + LR è®­ç»ƒå®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 2: 5æŠ˜ TF-IDF + LR ============================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ å¼€å§‹ TF-IDF + LogisticRegression 5æŠ˜äº¤å‰éªŒè¯\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def build_tfidf_lr():\n",
    "    vec = TfidfVectorizer(analyzer='char', ngram_range=(2,5),\n",
    "                          max_features=200000, lowercase=False)\n",
    "    clf = LogisticRegression(solver='saga', class_weight='balanced',\n",
    "                             max_iter=350, n_jobs=-1)\n",
    "    return Pipeline([(\"tfidf\", vec), (\"lr\", clf)])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_tfidf = np.zeros(len(X))\n",
    "pred_tfidf = np.zeros(len(X_test))\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(tqdm(skf.split(X, y), total=5, desc=\"5æŠ˜äº¤å‰éªŒè¯\"), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“¦ Fold {fold}/5 å¼€å§‹è®­ç»ƒ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   è®­ç»ƒé›†å¤§å°: {len(tr)} ({len(tr)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   éªŒè¯é›†å¤§å°: {len(va)} ({len(va)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y[tr])}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = build_tfidf_lr()\n",
    "    \n",
    "    print(\"   ğŸ”„ TF-IDFç‰¹å¾æå–å’Œæ¨¡å‹è®­ç»ƒä¸­...\")\n",
    "    model.fit(X[tr], y[tr])\n",
    "    \n",
    "    # è·å–ç‰¹å¾ç»´åº¦\n",
    "    n_features = model.named_steps['tfidf'].transform(X[tr][:1]).shape[1]\n",
    "    print(f\"   âœ… ç‰¹å¾ç»´åº¦: {n_features}\")\n",
    "    \n",
    "    # éªŒè¯é›†é¢„æµ‹\n",
    "    print(\"   ğŸ”„ éªŒè¯é›†é¢„æµ‹ä¸­...\")\n",
    "    oof_tfidf[va] = model.predict_proba(X[va])[:,1]\n",
    "    \n",
    "    # æµ‹è¯•é›†é¢„æµ‹\n",
    "    print(\"   ğŸ”„ æµ‹è¯•é›†é¢„æµ‹ä¸­...\")\n",
    "    pred_tfidf += model.predict_proba(X_test)[:,1] / 5\n",
    "    \n",
    "    # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡\n",
    "    va_pred_binary = (oof_tfidf[va] >= 0.5).astype(int)\n",
    "    va_f1 = f1_score(y[va], va_pred_binary)\n",
    "    va_acc = accuracy_score(y[va], va_pred_binary)\n",
    "    va_prec = precision_score(y[va], va_pred_binary, zero_division=0)\n",
    "    va_rec = recall_score(y[va], va_pred_binary, zero_division=0)\n",
    "    \n",
    "    fold_metrics.append({\n",
    "        'fold': fold,\n",
    "        'f1': va_f1,\n",
    "        'acc': va_acc,\n",
    "        'prec': va_prec,\n",
    "        'rec': va_rec\n",
    "    })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "    print(f\"   ğŸ“Š Fold {fold} éªŒè¯é›†æŒ‡æ ‡:\")\n",
    "    print(f\"      F1: {va_f1:.5f} | å‡†ç¡®ç‡: {va_acc:.5f} | ç²¾ç¡®ç‡: {va_prec:.5f} | å¬å›ç‡: {va_rec:.5f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“ˆ 5æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼Œè®¡ç®—æœ€ä½³é˜ˆå€¼...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "p, r, t = precision_recall_curve(y, oof_tfidf)\n",
    "f1 = 2*p*r/(p+r+1e-8)\n",
    "thr_tfidf = t[np.argmax(f1)]\n",
    "best_f1 = f1.max()\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ä½³é˜ˆå€¼: {thr_tfidf:.4f}\")\n",
    "print(f\"ğŸ¯ OOF F1 Score: {best_f1:.5f}\")\n",
    "\n",
    "# ä½¿ç”¨æœ€ä½³é˜ˆå€¼çš„æŒ‡æ ‡\n",
    "best_pred = (oof_tfidf >= thr_tfidf).astype(int)\n",
    "best_acc = accuracy_score(y, best_pred)\n",
    "best_prec = precision_score(y, best_pred, zero_division=0)\n",
    "best_rec = recall_score(y, best_pred, zero_division=0)\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨æœ€ä½³é˜ˆå€¼åçš„æ•´ä½“æŒ‡æ ‡:\")\n",
    "print(f\"   F1: {best_f1:.5f} | å‡†ç¡®ç‡: {best_acc:.5f} | ç²¾ç¡®ç‡: {best_prec:.5f} | å¬å›ç‡: {best_rec:.5f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š å„æŠ˜è¯¦ç»†æŒ‡æ ‡:\")\n",
    "for m in fold_metrics:\n",
    "    print(f\"   Fold {m['fold']}: F1={m['f1']:.5f}, Acc={m['acc']:.5f}, Prec={m['prec']:.5f}, Rec={m['rec']:.5f}\")\n",
    "\n",
    "print(f\"\\nâœ… TF-IDF + LR è®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eece44",
   "metadata": {},
   "source": [
    "## å›¾åƒç‰¹å¾æå–ï¼ˆè‡ªåŠ¨ç¼“å­˜ + GPUåŠ é€Ÿ + é¢„è®­ç»ƒæƒé‡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915abf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¼ï¸  å¼€å§‹å›¾åƒç‰¹å¾æå–\n",
      "============================================================\n",
      "ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: cuda\n",
      "   GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   æ˜¾å­˜: 8.00 GB\n",
      "\n",
      "ğŸ“¥ æå–è®­ç»ƒé›†å›¾åƒç‰¹å¾...\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/clip_train.npz\n",
      "   ğŸ“Š è®­ç»ƒé›† CLIPç‰¹å¾å½¢çŠ¶: (7000, 512)\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/deit_train.npz\n",
      "   ğŸ“Š è®­ç»ƒé›† DeiTç‰¹å¾å½¢çŠ¶: (7000, 768)\n",
      "\n",
      "ğŸ“¥ æå–æµ‹è¯•é›†å›¾åƒç‰¹å¾...\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/clip_test.npz\n",
      "   ğŸ“Š æµ‹è¯•é›† CLIPç‰¹å¾å½¢çŠ¶: (3000, 512)\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/deit_test.npz\n",
      "   ğŸ“Š æµ‹è¯•é›† DeiTç‰¹å¾å½¢çŠ¶: (3000, 768)\n",
      "\n",
      "ğŸ”— èåˆå›¾åƒç‰¹å¾...\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆå›¾åƒç‰¹å¾ç»Ÿè®¡:\n",
      "   è®­ç»ƒé›†: (7000, 1280) (CLIP: 512ç»´ + DeiT: 768ç»´)\n",
      "   æµ‹è¯•é›†: (3000, 1280) (CLIP: 512ç»´ + DeiT: 768ç»´)\n",
      "   ç‰¹å¾å‡å€¼: è®­ç»ƒé›†=-0.0083, æµ‹è¯•é›†=-0.0086\n",
      "   ç‰¹å¾æ ‡å‡†å·®: è®­ç»ƒé›†=0.6828, æµ‹è¯•é›†=0.6842\n",
      "\n",
      "âœ… å›¾åƒç‰¹å¾æå–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 3: å›¾åƒç‰¹å¾æå– ============================\n",
    "import cv2, torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import timm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¼ï¸  å¼€å§‹å›¾åƒç‰¹å¾æå–\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "def safe_load_img(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return img\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_clip(paths, save_path, dataset_name=\"\"):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"   âœ… åŠ è½½ç¼“å­˜: {save_path}\")\n",
    "        feats = np.load(save_path)['feats']\n",
    "        print(f\"   ğŸ“Š {dataset_name} CLIPç‰¹å¾å½¢çŠ¶: {feats.shape}\")\n",
    "        return feats\n",
    "    \n",
    "    print(f\"   ğŸ”„ {dataset_name} æå– CLIP ViT-B/16 ç‰¹å¾...\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='laion400m_e32')\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    print(f\"   ğŸ“¦ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "    \n",
    "    feats = []\n",
    "    failed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p in tqdm(paths, desc=f\"   CLIP {dataset_name}\", leave=False):\n",
    "            img = safe_load_img(os.path.join(\"image\", p))\n",
    "            if img is None:\n",
    "                feats.append(np.zeros(768, dtype=np.float32))\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            x = preprocess(img).unsqueeze(0).to(device)\n",
    "            f = model.encode_image(x).float().cpu().numpy()[0]\n",
    "            feats.append(f)\n",
    "    \n",
    "    feats = np.vstack(feats)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… {dataset_name} CLIPç‰¹å¾æå–å®Œæˆ:\")\n",
    "    print(f\"      ç‰¹å¾å½¢çŠ¶: {feats.shape}\")\n",
    "    print(f\"      å¤±è´¥å›¾åƒæ•°: {failed_count} ({failed_count/len(paths)*100:.2f}%)\")\n",
    "    print(f\"      è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/len(paths)*1000:.2f}ms/æ ·æœ¬)\")\n",
    "    print(f\"      ä¿å­˜åˆ°: {save_path}\")\n",
    "    \n",
    "    np.savez_compressed(save_path, feats=feats)\n",
    "    return feats\n",
    "\n",
    "def extract_deit(paths, save_path, dataset_name=\"\"):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"   âœ… åŠ è½½ç¼“å­˜: {save_path}\")\n",
    "        feats = np.load(save_path)['feats']\n",
    "        print(f\"   ğŸ“Š {dataset_name} DeiTç‰¹å¾å½¢çŠ¶: {feats.shape}\")\n",
    "        return feats\n",
    "    \n",
    "    print(f\"   ğŸ”„ {dataset_name} æå– DeiT-base ç‰¹å¾...\")\n",
    "    model = timm.create_model(\"deit_base_patch16_224.fb_in1k\",\n",
    "                               pretrained=True, num_classes=0).to(device).eval()\n",
    "    trans = timm.data.create_transform(**timm.data.resolve_model_data_config(model))\n",
    "    \n",
    "    print(f\"   ğŸ“¦ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "    \n",
    "    feats = []\n",
    "    failed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p in tqdm(paths, desc=f\"   DeiT {dataset_name}\", leave=False):\n",
    "            img = safe_load_img(os.path.join(\"image\", p))\n",
    "            if img is None:\n",
    "                feats.append(np.zeros(768, dtype=np.float32))\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            x = trans(img).unsqueeze(0).to(device)\n",
    "            f = model(x).cpu().numpy()[0]\n",
    "            feats.append(f)\n",
    "    \n",
    "    feats = np.vstack(feats)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"   âœ… {dataset_name} DeiTç‰¹å¾æå–å®Œæˆ:\")\n",
    "    print(f\"      ç‰¹å¾å½¢çŠ¶: {feats.shape}\")\n",
    "    print(f\"      å¤±è´¥å›¾åƒæ•°: {failed_count} ({failed_count/len(paths)*100:.2f}%)\")\n",
    "    print(f\"      è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/len(paths)*1000:.2f}ms/æ ·æœ¬)\")\n",
    "    print(f\"      ä¿å­˜åˆ°: {save_path}\")\n",
    "    \n",
    "    np.savez_compressed(save_path, feats=feats)\n",
    "    return feats\n",
    "\n",
    "print(\"\\nğŸ“¥ æå–è®­ç»ƒé›†å›¾åƒç‰¹å¾...\")\n",
    "clip_train = extract_clip(train_df['image_path'], f\"{CACHE}/clip_train.npz\", \"è®­ç»ƒé›†\")\n",
    "deit_train = extract_deit(train_df['image_path'], f\"{CACHE}/deit_train.npz\", \"è®­ç»ƒé›†\")\n",
    "\n",
    "print(\"\\nğŸ“¥ æå–æµ‹è¯•é›†å›¾åƒç‰¹å¾...\")\n",
    "clip_test  = extract_clip(test_df['image_path'],  f\"{CACHE}/clip_test.npz\", \"æµ‹è¯•é›†\")\n",
    "deit_test  = extract_deit(test_df['image_path'],  f\"{CACHE}/deit_test.npz\", \"æµ‹è¯•é›†\")\n",
    "\n",
    "print(\"\\nğŸ”— èåˆå›¾åƒç‰¹å¾...\")\n",
    "img_train = np.hstack([clip_train, deit_train])\n",
    "img_test  = np.hstack([clip_test,  deit_test])\n",
    "\n",
    "print(f\"\\nğŸ“Š æœ€ç»ˆå›¾åƒç‰¹å¾ç»Ÿè®¡:\")\n",
    "print(f\"   è®­ç»ƒé›†: {img_train.shape} (CLIP: {clip_train.shape[1]}ç»´ + DeiT: {deit_train.shape[1]}ç»´)\")\n",
    "print(f\"   æµ‹è¯•é›†: {img_test.shape} (CLIP: {clip_test.shape[1]}ç»´ + DeiT: {deit_test.shape[1]}ç»´)\")\n",
    "print(f\"   ç‰¹å¾å‡å€¼: è®­ç»ƒé›†={img_train.mean():.4f}, æµ‹è¯•é›†={img_test.mean():.4f}\")\n",
    "print(f\"   ç‰¹å¾æ ‡å‡†å·®: è®­ç»ƒé›†={img_train.std():.4f}, æµ‹è¯•é›†={img_test.std():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… å›¾åƒç‰¹å¾æå–å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14d389",
   "metadata": {},
   "source": [
    "## ç»ˆææ™šèåˆï¼ˆTFIDF + CLIP + ViT + RoBERTa-largeï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3bacad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— å¼€å§‹æ™šèåˆæ¨¡å‹è®­ç»ƒ (TF-IDF + å›¾åƒç‰¹å¾)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ TF-IDFç‰¹å¾æå–ä¸­...\n",
      "   âœ… æ–‡æœ¬ç‰¹å¾å½¢çŠ¶:\n",
      "      è®­ç»ƒé›†: (7000, 200000)\n",
      "      æµ‹è¯•é›†: (3000, 200000)\n",
      "      ç‰¹å¾ç»´åº¦: 200000\n",
      "\n",
      "ğŸ”— èåˆæ–‡æœ¬å’Œå›¾åƒç‰¹å¾...\n",
      "   âœ… èåˆåç‰¹å¾å½¢çŠ¶:\n",
      "      è®­ç»ƒé›†: (7000, 201280) (æ–‡æœ¬: 200000 + å›¾åƒ: 1280 = 201280)\n",
      "      æµ‹è¯•é›†: (3000, 201280)\n",
      "      ç¨€ç–çŸ©é˜µæ ¼å¼: csr, éé›¶å…ƒç´ æ¯”ä¾‹: 1.4459%\n",
      "\n",
      "ğŸš€ è®­ç»ƒèåˆæ¨¡å‹...\n",
      "   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: 134.31ç§’\n",
      "   ğŸ“Š æ¨¡å‹ç³»æ•°ç»Ÿè®¡:\n",
      "      ç³»æ•°å‡å€¼: 0.0034\n",
      "      ç³»æ•°æ ‡å‡†å·®: 0.0473\n",
      "      éé›¶ç³»æ•°: 201280 / 201280\n",
      "\n",
      "ğŸ”„ ç”Ÿæˆé¢„æµ‹æ¦‚ç‡...\n",
      "\n",
      "ğŸ“Š èåˆæ¨¡å‹æ€§èƒ½:\n",
      "   æœ€ä½³é˜ˆå€¼: 0.4647\n",
      "   è®­ç»ƒé›† F1: 0.86329\n",
      "   é¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:\n",
      "      å‡å€¼: 0.4479\n",
      "      æ ‡å‡†å·®: 0.3606\n",
      "      æœ€å°å€¼: 0.0000\n",
      "      æœ€å¤§å€¼: 0.9997\n",
      "\n",
      "âœ… æ™šèåˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 4: æ™šèåˆ ============================\n",
    "from scipy import sparse\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— å¼€å§‹æ™šèåˆæ¨¡å‹è®­ç»ƒ (TF-IDF + å›¾åƒç‰¹å¾)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ”„ TF-IDFç‰¹å¾æå–ä¸­...\")\n",
    "vec = TfidfVectorizer(analyzer='char', ngram_range=(2,5), max_features=200000)\n",
    "text_train = vec.fit_transform(X)\n",
    "text_test  = vec.transform(X_test)\n",
    "\n",
    "print(f\"   âœ… æ–‡æœ¬ç‰¹å¾å½¢çŠ¶:\")\n",
    "print(f\"      è®­ç»ƒé›†: {text_train.shape}\")\n",
    "print(f\"      æµ‹è¯•é›†: {text_test.shape}\")\n",
    "print(f\"      ç‰¹å¾ç»´åº¦: {text_train.shape[1]}\")\n",
    "\n",
    "print(\"\\nğŸ”— èåˆæ–‡æœ¬å’Œå›¾åƒç‰¹å¾...\")\n",
    "Xfull = sparse.hstack([text_train, sparse.csr_matrix(img_train)])\n",
    "Xtest = sparse.hstack([text_test,  sparse.csr_matrix(img_test)])\n",
    "\n",
    "print(f\"   âœ… èåˆåç‰¹å¾å½¢çŠ¶:\")\n",
    "print(f\"      è®­ç»ƒé›†: {Xfull.shape} (æ–‡æœ¬: {text_train.shape[1]} + å›¾åƒ: {img_train.shape[1]} = {Xfull.shape[1]})\")\n",
    "print(f\"      æµ‹è¯•é›†: {Xtest.shape}\")\n",
    "print(f\"      ç¨€ç–çŸ©é˜µæ ¼å¼: {Xfull.format}, éé›¶å…ƒç´ æ¯”ä¾‹: {Xfull.nnz / (Xfull.shape[0] * Xfull.shape[1]) * 100:.4f}%\")\n",
    "\n",
    "print(\"\\nğŸš€ è®­ç»ƒèåˆæ¨¡å‹...\")\n",
    "start_time = time.time()\n",
    "clf_fusion = LogisticRegression(solver='saga', class_weight='balanced',\n",
    "                                max_iter=500, n_jobs=-1)\n",
    "clf_fusion.fit(Xfull, y)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "print(f\"   ğŸ“Š æ¨¡å‹ç³»æ•°ç»Ÿè®¡:\")\n",
    "coef = clf_fusion.coef_[0]\n",
    "print(f\"      ç³»æ•°å‡å€¼: {coef.mean():.4f}\")\n",
    "print(f\"      ç³»æ•°æ ‡å‡†å·®: {coef.std():.4f}\")\n",
    "print(f\"      éé›¶ç³»æ•°: {(coef != 0).sum()} / {len(coef)}\")\n",
    "\n",
    "print(\"\\nğŸ”„ ç”Ÿæˆé¢„æµ‹æ¦‚ç‡...\")\n",
    "prob_fusion = clf_fusion.predict_proba(Xtest)[:,1]\n",
    "train_prob_fusion = clf_fusion.predict_proba(Xfull)[:,1]\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒé›†æŒ‡æ ‡\n",
    "p, r, t = precision_recall_curve(y, train_prob_fusion)\n",
    "f1 = 2*p*r/(p+r+1e-8)\n",
    "thr_fusion = t[np.argmax(f1)]\n",
    "best_f1_fusion = f1.max()\n",
    "\n",
    "print(f\"\\nğŸ“Š èåˆæ¨¡å‹æ€§èƒ½:\")\n",
    "print(f\"   æœ€ä½³é˜ˆå€¼: {thr_fusion:.4f}\")\n",
    "print(f\"   è®­ç»ƒé›† F1: {best_f1_fusion:.5f}\")\n",
    "print(f\"   é¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:\")\n",
    "print(f\"      å‡å€¼: {prob_fusion.mean():.4f}\")\n",
    "print(f\"      æ ‡å‡†å·®: {prob_fusion.std():.4f}\")\n",
    "print(f\"      æœ€å°å€¼: {prob_fusion.min():.4f}\")\n",
    "print(f\"      æœ€å¤§å€¼: {prob_fusion.max():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… æ™šèåˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7c5e5",
   "metadata": {},
   "source": [
    "## RoBERTa-large 5æŠ˜ + è‡ªåŠ¨ç¼“å­˜ + æœ€ç»ˆåŠ æƒèåˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328ba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¤– å¼€å§‹ RoBERTa 5æŠ˜äº¤å‰éªŒè¯\n",
      "============================================================\n",
      "ğŸ–¥ï¸  æ“ä½œç³»ç»Ÿ: Linux/WSL2\n",
      "ğŸ–¥ï¸  GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "   æ˜¾å­˜: 8.00 GB\n",
      "   âœ… å·²å¯ç”¨TF32åŠ é€Ÿ\n",
      "   ğŸ’¡ æ˜¾å­˜å……è¶³ï¼Œä½¿ç”¨largeç‰ˆæœ¬ï¼ˆå¦‚éœ€æ›´å¿«é€Ÿåº¦ï¼Œå¯æ‰‹åŠ¨æ”¹ä¸ºbaseç‰ˆæœ¬ï¼‰\n",
      "\n",
      "ğŸ“¥ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: hfl/chinese-roberta-wwm-ext-large\n",
      "   âœ… è¯æ±‡è¡¨å¤§å°: 21128\n",
      "\n",
      "ğŸ”„ é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬æ•°æ®...\n",
      "   âœ… è®­ç»ƒé›†tokenized: torch.Size([7000, 512])\n",
      "   âœ… æµ‹è¯•é›†tokenized: torch.Size([3000, 512])\n",
      "\n",
      "============================================================\n",
      "ğŸ“¦ Fold 1/5 å¼€å§‹\n",
      "============================================================\n",
      "   è®­ç»ƒé›†: 5600 æ ·æœ¬ (80.0%)\n",
      "   éªŒè¯é›†: 1400 æ ·æœ¬ (20.0%)\n",
      "   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: [3309 2291]\n",
      "\n",
      "   ğŸ“¥ åŠ è½½æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… æ¨¡å‹å‚æ•°é‡: 325.5M (å¯è®­ç»ƒ: 325.5M)\n",
      "   âœ… ä½¿ç”¨fused AdamWä¼˜åŒ–å™¨\n",
      "   âš ï¸  å·²ç¦ç”¨torch.compileï¼ˆé¿å…embeddingç›¸å…³æŠ¥é”™ï¼Œé€Ÿåº¦å½±å“å¾ˆå°ï¼‰\n",
      "   ğŸ“Š è®­ç»ƒé…ç½®:\n",
      "      æ‰¹æ¬¡å¤§å°: 16 (è®­ç»ƒ), 32 (éªŒè¯/æµ‹è¯•)\n",
      "      æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 2 (æœ‰æ•ˆæ‰¹æ¬¡: 32)\n",
      "      æ•°æ®åŠ è½½è¿›ç¨‹æ•°: 2\n",
      "      æ€»æ­¥æ•°: 350\n",
      "      å­¦ä¹ ç‡: 3e-5\n",
      "      Warmupæ­¥æ•°: 50\n",
      "\n",
      "   ğŸ”„ Epoch 1/2 è®­ç»ƒä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Fold 1 Epoch 1:   3%|â–         | 10/350 [09:00<5:27:39, 57.82s/it, loss=0.6359, lr=3.00e-06, gpu=3.0GB, step=10/350]"
     ]
    }
   ],
   "source": [
    "# ============================ PART 5: RoBERTaï¼ˆWindowsä¼˜åŒ–é€Ÿåº¦ç‰ˆæœ¬ï¼‰ ============================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import amp\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¤– å¼€å§‹ RoBERTa 5æŠ˜äº¤å‰éªŒè¯\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ£€æµ‹æ“ä½œç³»ç»Ÿ\n",
    "is_windows = os.name == 'nt'\n",
    "print(f\"ğŸ–¥ï¸  æ“ä½œç³»ç»Ÿ: {'Windows' if is_windows else 'Linux/WSL2'}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_memory = 0\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"   æ˜¾å­˜: {gpu_memory:.2f} GB\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # å¯ç”¨TF32åŠ é€Ÿï¼ˆAmpereæ¶æ„åŠä»¥ä¸Šï¼‰\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"   âœ… å·²å¯ç”¨TF32åŠ é€Ÿ\")\n",
    "    \n",
    "    # Windowsç‰¹å®šä¼˜åŒ–ï¼šè®¾ç½®CUDAè®¾å¤‡å±æ€§\n",
    "    if is_windows:\n",
    "        # ç¦ç”¨CUDAå›¾ä¼˜åŒ–ï¼ˆWindowsä¸Šå¯èƒ½æœ‰é—®é¢˜ï¼‰\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "        print(\"   âœ… å·²åº”ç”¨Windows CUDAä¼˜åŒ–\")\n",
    "else:\n",
    "    print(\"   âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰\")\n",
    "\n",
    "# ========== æ¨¡å‹é€‰æ‹©ï¼šWindowsä¸Šä¼˜å…ˆä½¿ç”¨baseç‰ˆæœ¬ ==========\n",
    "# é€‰é¡¹1: baseç‰ˆæœ¬ï¼ˆ125Må‚æ•°ï¼Œé€Ÿåº¦å¿«2-3å€ï¼Œæ€§èƒ½ç•¥é™ï¼‰\n",
    "# MODEL = \"hfl/chinese-roberta-wwm-ext\"  # baseç‰ˆæœ¬ï¼Œæ›´å¿«\n",
    "# é€‰é¡¹2: largeç‰ˆæœ¬ï¼ˆ330Må‚æ•°ï¼Œé€Ÿåº¦æ…¢ä½†æ€§èƒ½æ›´å¥½ï¼‰\n",
    "MODEL = \"hfl/chinese-roberta-wwm-ext-large\"  # largeç‰ˆæœ¬\n",
    "\n",
    "# Windowsä¸Šè‡ªåŠ¨ä½¿ç”¨baseç‰ˆæœ¬ä»¥æå‡é€Ÿåº¦ï¼ˆé™¤éæ˜¾å­˜å……è¶³ä¸”ç”¨æˆ·æ˜ç¡®éœ€è¦largeï¼‰\n",
    "if is_windows:\n",
    "    if torch.cuda.is_available() and gpu_memory >= 10:\n",
    "        print(f\"   ğŸ’¡ Windowsç¯å¢ƒï¼Œæ˜¾å­˜å……è¶³ï¼Œä½¿ç”¨largeç‰ˆæœ¬\")\n",
    "    else:\n",
    "        MODEL = \"hfl/chinese-roberta-wwm-ext\"\n",
    "        print(f\"   âš¡ Windowsç¯å¢ƒï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°baseç‰ˆæœ¬ä»¥æå‡é€Ÿåº¦ï¼ˆå¦‚éœ€largeç‰ˆæœ¬å¯æ‰‹åŠ¨ä¿®æ”¹ï¼‰\")\n",
    "elif torch.cuda.is_available() and gpu_memory < 8:\n",
    "    MODEL = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    print(f\"   âš ï¸  æ˜¾å­˜ < 8GBï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°baseç‰ˆæœ¬ä»¥æå‡é€Ÿåº¦\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(f\"   ğŸ’¡ æ˜¾å­˜å……è¶³ï¼Œä½¿ç”¨largeç‰ˆæœ¬ï¼ˆå¦‚éœ€æ›´å¿«é€Ÿåº¦ï¼Œå¯æ‰‹åŠ¨æ”¹ä¸ºbaseç‰ˆæœ¬ï¼‰\")\n",
    "\n",
    "print(f\"\\nğŸ“¥ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨: {MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "print(f\"   âœ… è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}\")\n",
    "\n",
    "def tokenize_all(texts):\n",
    "    return tokenizer(list(texts), truncation=True, max_length=512,\n",
    "                     padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"\\nğŸ”„ é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬æ•°æ®...\")\n",
    "train_enc = tokenize_all(X)\n",
    "test_enc  = tokenize_all(X_test)\n",
    "print(f\"   âœ… è®­ç»ƒé›†tokenized: {train_enc['input_ids'].shape}\")\n",
    "print(f\"   âœ… æµ‹è¯•é›†tokenized: {test_enc['input_ids'].shape}\")\n",
    "\n",
    "class MySet(Dataset):\n",
    "    def __init__(self, enc, y=None):\n",
    "        self.enc = enc\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.enc['input_ids'])\n",
    "    def __getitem__(self, i):\n",
    "        item = {k: v[i] for k,v in self.enc.items()}\n",
    "        if self.y is not None:\n",
    "            item['labels'] = torch.tensor(self.y[i],dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=SEED)\n",
    "pred_roberta = np.zeros(len(X_test))\n",
    "oof_roberta  = np.zeros(len(X))\n",
    "\n",
    "fold_summaries = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ“¦ Fold {fold}/5 å¼€å§‹\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   è®­ç»ƒé›†: {len(tr)} æ ·æœ¬ ({len(tr)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   éªŒè¯é›†: {len(va)} æ ·æœ¬ ({len(va)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y[tr])}\")\n",
    "    \n",
    "    fold_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n   ğŸ“¥ åŠ è½½æ¨¡å‹...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL, num_labels=2, dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   âœ… æ¨¡å‹å‚æ•°é‡: {n_params/1e6:.1f}M (å¯è®­ç»ƒ: {trainable_params/1e6:.1f}M)\")\n",
    "\n",
    "    train_ds = MySet({k:v[tr] for k,v in train_enc.items()}, y[tr])\n",
    "    val_ds   = MySet({k:v[va] for k,v in train_enc.items()}, y[va])\n",
    "    test_ds  = MySet(test_enc)\n",
    "\n",
    "    # ========== æ‰¹æ¬¡å¤§å°ä¼˜åŒ–ï¼šWindowsä¸Šä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½® ==========\n",
    "    if not torch.cuda.is_available():\n",
    "        # CPUæ¨¡å¼ï¼šå°æ‰¹æ¬¡\n",
    "        train_batch_size = 4\n",
    "        val_batch_size = 8\n",
    "        grad_accum_steps = 1\n",
    "    elif is_windows:\n",
    "        # Windowsç¯å¢ƒï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°ä»¥é¿å…é—®é¢˜\n",
    "        if \"large\" in MODEL.lower():\n",
    "            train_batch_size = 20 if gpu_memory >= 10 else 14  # Windowsä¸Šç¨å°\n",
    "            val_batch_size = 40 if gpu_memory >= 10 else 28\n",
    "            grad_accum_steps = 2\n",
    "        else:\n",
    "            train_batch_size = 28 if gpu_memory >= 8 else 20  # Windowsä¸Šç¨å°\n",
    "            val_batch_size = 56 if gpu_memory >= 8 else 40\n",
    "            grad_accum_steps = 1\n",
    "    elif \"large\" in MODEL.lower():\n",
    "        # Linux/WSL2: largeæ¨¡å‹\n",
    "        train_batch_size = 24 if gpu_memory >= 10 else 16\n",
    "        val_batch_size = 48 if gpu_memory >= 10 else 32\n",
    "        grad_accum_steps = 2\n",
    "    else:\n",
    "        # Linux/WSL2: baseæ¨¡å‹\n",
    "        train_batch_size = 32 if gpu_memory >= 8 else 24\n",
    "        val_batch_size = 64 if gpu_memory >= 8 else 48\n",
    "        grad_accum_steps = 1\n",
    "    \n",
    "    # Windowsä¸Šå¿…é¡»ä½¿ç”¨num_workers=0ï¼Œå¦åˆ™ä¼šæœ‰é—®é¢˜\n",
    "    num_workers = 0 if is_windows else 2\n",
    "    \n",
    "    # Windowsä¸Šç®€åŒ–DataLoaderé…ç½®\n",
    "    if is_windows:\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=train_batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=0, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, \n",
    "            batch_size=val_batch_size, \n",
    "            num_workers=0, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=val_batch_size, \n",
    "            num_workers=0, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "    else:\n",
    "        # Linux/WSL2: ä½¿ç”¨ä¼˜åŒ–çš„DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            train_ds, \n",
    "            batch_size=train_batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds, \n",
    "            batch_size=val_batch_size, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=val_batch_size, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2\n",
    "        )\n",
    "\n",
    "    # Windowsä¸Šä¸ä½¿ç”¨fusedä¼˜åŒ–å™¨ï¼ˆå¯èƒ½ä¸æ”¯æŒï¼‰\n",
    "    if is_windows:\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "        print(f\"   âš ï¸  Windowsç¯å¢ƒï¼Œä½¿ç”¨æ ‡å‡†AdamWï¼ˆfusedåœ¨Windowsä¸Šå¯èƒ½ä¸ç¨³å®šï¼‰\")\n",
    "    else:\n",
    "        try:\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01, fused=True)\n",
    "            print(f\"   âœ… ä½¿ç”¨fused AdamWä¼˜åŒ–å™¨\")\n",
    "        except:\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "            print(f\"   âš ï¸  fused AdamWä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†AdamW\")\n",
    "    \n",
    "    steps = len(train_loader) * 2 // grad_accum_steps\n",
    "    sched = get_linear_schedule_with_warmup(opt, 50, steps)\n",
    "    \n",
    "    # Windowsä¸Šä¸ä½¿ç”¨torch.compileï¼ˆé€šå¸¸å¾ˆæ…¢æˆ–ä¸ç¨³å®šï¼‰\n",
    "    use_compile = False\n",
    "    # print(f\"   âš ï¸  å·²ç¦ç”¨torch.compileï¼ˆé¿å…embeddingç›¸å…³æŠ¥é”™ï¼Œé€Ÿåº¦å½±å“å¾ˆå°ï¼‰\")\n",
    "    if hasattr(torch, 'compile') and not is_windows:\n",
    "        try:\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "            use_compile = True\n",
    "            print(f\"   âœ… å·²å¯ç”¨torch.compileåŠ é€Ÿ\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  torch.compileä¸å¯ç”¨: {e}\")\n",
    "    elif is_windows:\n",
    "        print(f\"   âš ï¸  Windowsç¯å¢ƒï¼Œè·³è¿‡torch.compileï¼ˆWindowsä¸Šé€šå¸¸å¾ˆæ…¢ï¼‰\")\n",
    "    \n",
    "    print(f\"   ğŸ“Š è®­ç»ƒé…ç½®:\")\n",
    "    print(f\"      æ‰¹æ¬¡å¤§å°: {train_batch_size} (è®­ç»ƒ), {val_batch_size} (éªŒè¯/æµ‹è¯•)\")\n",
    "    print(f\"      æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {grad_accum_steps} (æœ‰æ•ˆæ‰¹æ¬¡: {train_batch_size * grad_accum_steps})\")\n",
    "    print(f\"      æ•°æ®åŠ è½½è¿›ç¨‹æ•°: {num_workers}\")\n",
    "    print(f\"      æ€»æ­¥æ•°: {steps}\")\n",
    "    print(f\"      å­¦ä¹ ç‡: 3e-5\")\n",
    "    print(f\"      Warmupæ­¥æ•°: 50\")\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_epoch = -1\n",
    "    epoch_metrics = []\n",
    "\n",
    "    for epoch in range(2):\n",
    "        print(f\"\\n   ğŸ”„ Epoch {epoch+1}/2 è®­ç»ƒä¸­...\")\n",
    "        model.train()\n",
    "        \n",
    "        epoch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"      Fold {fold} Epoch {epoch+1}\", leave=False)\n",
    "        opt.zero_grad()  # åˆå§‹åŒ–æ¢¯åº¦\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Windowsä¸Šä½¿ç”¨non_blocking=Falseå¯èƒ½æ›´ç¨³å®š\n",
    "            non_blocking = not is_windows\n",
    "            batch = {k:v.to(device, non_blocking=non_blocking) for k,v in batch.items()}\n",
    "            \n",
    "            with amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                loss = model(**batch).loss / grad_accum_steps  # æ¢¯åº¦ç´¯ç§¯æ—¶ç¼©æ”¾æŸå¤±\n",
    "            \n",
    "            loss.backward()\n",
    "            epoch_losses.append(loss.item() * grad_accum_steps)  # è®°å½•çœŸå®æŸå¤±\n",
    "            \n",
    "            # æ¢¯åº¦ç´¯ç§¯ï¼šæ¯grad_accum_stepsæ­¥æ›´æ–°ä¸€æ¬¡\n",
    "            if (step + 1) % grad_accum_steps == 0 or (step + 1) == len(train_loader):\n",
    "                opt.step()\n",
    "                sched.step()\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                # Windowsä¸Šå®šæœŸæ¸…ç†ç¼“å­˜ä»¥é¿å…å†…å­˜ç¢ç‰‡\n",
    "                if is_windows and (step + 1) % (grad_accum_steps * 10) == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            current_lr = sched.get_last_lr()[0]\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                gpu_mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{epoch_losses[-1]:.4f}',\n",
    "                    'lr': f'{current_lr:.2e}',\n",
    "                    'gpu': f'{gpu_mem:.1f}GB',\n",
    "                    'step': f'{step+1}/{len(train_loader)}'\n",
    "                })\n",
    "            else:\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{epoch_losses[-1]:.4f}', \n",
    "                    'lr': f'{current_lr:.2e}',\n",
    "                    'step': f'{step+1}/{len(train_loader)}'\n",
    "                })\n",
    "        \n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"      âœ… Epoch {epoch+1} å®Œæˆ: å¹³å‡æŸå¤±={avg_loss:.4f}, è€—æ—¶={epoch_time:.1f}ç§’\")\n",
    "\n",
    "        # --------- éªŒè¯ ---------\n",
    "        print(f\"   ğŸ” Epoch {epoch+1} éªŒè¯ä¸­...\")\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            non_blocking = not is_windows\n",
    "            for batch in tqdm(val_loader, desc=\"      éªŒè¯\", leave=False):\n",
    "                b = {k:v.to(device, non_blocking=non_blocking) for k,v in batch.items()}\n",
    "                with amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    logits = model(**b).logits\n",
    "                p = torch.softmax(logits, dim=-1)[:,1]\n",
    "                # bfloat16éœ€è¦å…ˆè½¬æ¢ä¸ºfloat32æ‰èƒ½è½¬numpy\n",
    "                preds.append(p.float().cpu().numpy())\n",
    "                trues.append(b['labels'].cpu().numpy())\n",
    "        \n",
    "        preds = np.concatenate(preds)\n",
    "        trues = np.concatenate(trues)\n",
    "        \n",
    "        # è®¡ç®—å¤šä¸ªæŒ‡æ ‡\n",
    "        pred_binary = (preds >= 0.5).astype(int)\n",
    "        f1 = f1_score(trues, pred_binary)\n",
    "        acc = accuracy_score(trues, pred_binary)\n",
    "        prec = precision_score(trues, pred_binary, zero_division=0)\n",
    "        rec = recall_score(trues, pred_binary, zero_division=0)\n",
    "        \n",
    "        epoch_metrics.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss': avg_loss,\n",
    "            'f1': f1,\n",
    "            'acc': acc,\n",
    "            'prec': prec,\n",
    "            'rec': rec\n",
    "        })\n",
    "        \n",
    "        print(f\"      ğŸ“Š Epoch {epoch+1} éªŒè¯æŒ‡æ ‡:\")\n",
    "        print(f\"         F1: {f1:.5f} | å‡†ç¡®ç‡: {acc:.5f} | ç²¾ç¡®ç‡: {prec:.5f} | å¬å›ç‡: {rec:.5f}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_epoch = epoch + 1\n",
    "            # å¦‚æœä½¿ç”¨äº†compileï¼Œéœ€è¦è·å–åŸå§‹æ¨¡å‹çš„state_dict\n",
    "            if use_compile:\n",
    "                # torch.compileåçš„æ¨¡å‹æœ‰_orig_modå±æ€§ï¼Œä¿å­˜åŸå§‹æ¨¡å‹çš„æƒé‡\n",
    "                state_dict_to_save = model._orig_mod.state_dict()\n",
    "            else:\n",
    "                state_dict_to_save = model.state_dict()\n",
    "            torch.save(state_dict_to_save, f\"{CACHE}/best_model_fold{fold}.pt\")\n",
    "            print(f\"      ğŸ”¥ ä¿å­˜æœ€ä½³æ¨¡å‹ (F1={f1:.5f})\")\n",
    "\n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    print(f\"\\n   ğŸ“¥ åŠ è½½æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})...\")\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL, num_labels=2, dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "    \n",
    "    # åŠ è½½state_dictå¹¶å¤„ç†å¯èƒ½çš„_orig_modå‰ç¼€\n",
    "    saved_state_dict = torch.load(f\"{CACHE}/best_model_fold{fold}.pt\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰_orig_modå‰ç¼€ï¼ˆå¯èƒ½æ˜¯ä¹‹å‰ä¿å­˜çš„compileæ¨¡å‹ï¼‰\n",
    "    if any(k.startswith('_orig_mod.') for k in saved_state_dict.keys()):\n",
    "        # ç§»é™¤_orig_modå‰ç¼€\n",
    "        new_state_dict = {}\n",
    "        for k, v in saved_state_dict.items():\n",
    "            if k.startswith('_orig_mod.'):\n",
    "                new_state_dict[k.replace('_orig_mod.', '')] = v\n",
    "            else:\n",
    "                new_state_dict[k] = v\n",
    "        saved_state_dict = new_state_dict\n",
    "        print(f\"   âœ… å·²å¤„ç†state_dictä¸­çš„_orig_modå‰ç¼€\")\n",
    "    \n",
    "    best_model.load_state_dict(saved_state_dict, strict=False)\n",
    "    \n",
    "    # å¦‚æœä½¿ç”¨äº†compileï¼Œä¹Ÿéœ€è¦compileæœ€ä½³æ¨¡å‹\n",
    "    if use_compile:\n",
    "        best_model = torch.compile(best_model, mode='reduce-overhead')\n",
    "\n",
    "    # éªŒè¯é›†æ¦‚ç‡\n",
    "    print(f\"   ğŸ”„ ç”ŸæˆéªŒè¯é›†é¢„æµ‹...\")\n",
    "    best_model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        non_blocking = not is_windows\n",
    "        for batch in tqdm(val_loader, desc=\"      éªŒè¯é›†é¢„æµ‹\", leave=False):\n",
    "            b = {k:v.to(device, non_blocking=non_blocking) for k,v in batch.items()}\n",
    "            with amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = best_model(**b).logits\n",
    "            # bfloat16éœ€è¦å…ˆè½¬æ¢ä¸ºfloat32æ‰èƒ½è½¬numpy\n",
    "            preds.append(torch.softmax(logits, dim=-1)[:,1].float().cpu().numpy())\n",
    "    oof_roberta[va] = np.concatenate(preds)\n",
    "    \n",
    "    # è®¡ç®—éªŒè¯é›†æœ€ä½³é˜ˆå€¼\n",
    "    p, r, t = precision_recall_curve(y[va], oof_roberta[va])\n",
    "    f1_curve = 2*p*r/(p+r+1e-12)\n",
    "    best_thr = t[np.argmax(f1_curve)]\n",
    "    best_f1_thr = f1_curve.max()\n",
    "    print(f\"   ğŸ¯ éªŒè¯é›†æœ€ä½³é˜ˆå€¼: {best_thr:.4f}, F1: {best_f1_thr:.5f}\")\n",
    "\n",
    "    # test é¢„æµ‹\n",
    "    print(f\"   ğŸ”„ ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...\")\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        non_blocking = not is_windows\n",
    "        for batch in tqdm(test_loader, desc=\"      æµ‹è¯•é›†é¢„æµ‹\", leave=False):\n",
    "            b = {k:v.to(device, non_blocking=non_blocking) for k,v in batch.items()}\n",
    "            with amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logits = best_model(**b).logits\n",
    "            # bfloat16éœ€è¦å…ˆè½¬æ¢ä¸ºfloat32æ‰èƒ½è½¬numpy\n",
    "            preds.append(torch.softmax(logits, dim=-1)[:,1].float().cpu().numpy())\n",
    "    pred_roberta += np.concatenate(preds) / 5\n",
    "    \n",
    "    fold_time = time.time() - fold_start_time\n",
    "    print(f\"\\n   âœ… Fold {fold} å®Œæˆï¼Œæ€»è€—æ—¶: {fold_time/60:.1f}åˆ†é’Ÿ\")\n",
    "    print(f\"   ğŸ“Š Fold {fold} æœ€ä½³æŒ‡æ ‡ (Epoch {best_epoch}):\")\n",
    "    best_epoch_metrics = epoch_metrics[best_epoch-1]\n",
    "    print(f\"      F1: {best_epoch_metrics['f1']:.5f} | å‡†ç¡®ç‡: {best_epoch_metrics['acc']:.5f}\")\n",
    "    print(f\"      ç²¾ç¡®ç‡: {best_epoch_metrics['prec']:.5f} | å¬å›ç‡: {best_epoch_metrics['rec']:.5f}\")\n",
    "    \n",
    "    fold_summaries.append({\n",
    "        'fold': fold,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_f1': best_f1,\n",
    "        'best_thr': best_thr,\n",
    "        'time': fold_time\n",
    "    })\n",
    "    \n",
    "    # æ¸…ç†æ˜¾å­˜\n",
    "    del model, best_model, opt, sched, train_loader, val_loader, test_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“ˆ RoBERTa 5æŠ˜äº¤å‰éªŒè¯å®Œæˆ\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nğŸ“Š å„æŠ˜æ€»ç»“:\")\n",
    "for s in fold_summaries:\n",
    "    print(f\"   Fold {s['fold']}: æœ€ä½³Epoch={s['best_epoch']}, F1={s['best_f1']:.5f}, \"\n",
    "          f\"æœ€ä½³é˜ˆå€¼={s['best_thr']:.4f}, è€—æ—¶={s['time']/60:.1f}åˆ†é’Ÿ\")\n",
    "\n",
    "# è®¡ç®—æ•´ä½“OOFæŒ‡æ ‡\n",
    "p, r, t = precision_recall_curve(y, oof_roberta)\n",
    "f1_curve = 2*p*r/(p+r+1e-12)\n",
    "best_thr_roberta = t[np.argmax(f1_curve)]\n",
    "best_f1_roberta = f1_curve.max()\n",
    "\n",
    "print(f\"\\nğŸ¯ RoBERTaæ•´ä½“OOFæŒ‡æ ‡:\")\n",
    "print(f\"   æœ€ä½³é˜ˆå€¼: {best_thr_roberta:.4f}\")\n",
    "print(f\"   OOF F1: {best_f1_roberta:.5f}\")\n",
    "print(f\"   é¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:\")\n",
    "print(f\"      å‡å€¼: {pred_roberta.mean():.4f}\")\n",
    "print(f\"      æ ‡å‡†å·®: {pred_roberta.std():.4f}\")\n",
    "\n",
    "print(\"\\nâœ… RoBERTaè®­ç»ƒå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84762b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— æœ€ç»ˆæ¨¡å‹èåˆ\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š èåˆæƒé‡:\n",
      "   æ™šèåˆæ¨¡å‹ (TF-IDF + å›¾åƒ): 34%\n",
      "   RoBERTaæ¨¡å‹: 66%\n",
      "\n",
      "ğŸ“ˆ èåˆåé¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:\n",
      "   å‡å€¼: 0.4326\n",
      "   æ ‡å‡†å·®: 0.2734\n",
      "   æœ€å°å€¼: 0.0400\n",
      "   æœ€å¤§å€¼: 0.9086\n",
      "   ä¸­ä½æ•°: 0.3627\n",
      "\n",
      "ğŸ¯ æœ€ä½³é˜ˆå€¼åˆ†æ:\n",
      "   æœ€ä½³é˜ˆå€¼: 0.3666\n",
      "   è®­ç»ƒé›†F1: 0.82828\n",
      "\n",
      "ğŸ“Š ä½¿ç”¨é˜ˆå€¼ 0.52 çš„é¢„æµ‹ç»Ÿè®¡:\n",
      "   é¢„æµ‹ä¸ºæ­£ç±»: 982 (32.73%)\n",
      "   é¢„æµ‹ä¸ºè´Ÿç±»: 2018 (67.27%)\n",
      "\n",
      "âœ… å·²ä¿å­˜æœ€ç»ˆæäº¤æ–‡ä»¶: FINAL_SUBMISSION.csv\n",
      "   æ ·æœ¬æ•°: 3000\n",
      "   æ­£ç±»æ•°: 982\n",
      "   è´Ÿç±»æ•°: 2018\n",
      "\n",
      "ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 6: æœ€ç»ˆèåˆ ============================\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— æœ€ç»ˆæ¨¡å‹èåˆ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š èåˆæƒé‡:\")\n",
    "print(\"   æ™šèåˆæ¨¡å‹ (TF-IDF + å›¾åƒ): 34%\")\n",
    "print(\"   RoBERTaæ¨¡å‹: 66%\")\n",
    "\n",
    "final_prob = 0.34*prob_fusion + 0.66*pred_roberta\n",
    "\n",
    "print(f\"\\nğŸ“ˆ èåˆåé¢„æµ‹æ¦‚ç‡ç»Ÿè®¡:\")\n",
    "print(f\"   å‡å€¼: {final_prob.mean():.4f}\")\n",
    "print(f\"   æ ‡å‡†å·®: {final_prob.std():.4f}\")\n",
    "print(f\"   æœ€å°å€¼: {final_prob.min():.4f}\")\n",
    "print(f\"   æœ€å¤§å€¼: {final_prob.max():.4f}\")\n",
    "print(f\"   ä¸­ä½æ•°: {np.median(final_prob):.4f}\")\n",
    "\n",
    "# ä½¿ç”¨è®­ç»ƒé›†èåˆæ¦‚ç‡æ‰¾æœ€ä½³é˜ˆå€¼\n",
    "train_final_prob = 0.34*train_prob_fusion + 0.66*oof_roberta\n",
    "p, r, t = precision_recall_curve(y, train_final_prob)\n",
    "f1_curve = 2*p*r/(p+r+1e-12)\n",
    "best_thr_final = t[np.argmax(f1_curve)]\n",
    "best_f1_final = f1_curve.max()\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€ä½³é˜ˆå€¼åˆ†æ:\")\n",
    "print(f\"   æœ€ä½³é˜ˆå€¼: {best_thr_final:.4f}\")\n",
    "print(f\"   è®­ç»ƒé›†F1: {best_f1_final:.5f}\")\n",
    "\n",
    "# ä½¿ç”¨0.52é˜ˆå€¼ï¼ˆç”¨æˆ·æŒ‡å®šï¼‰\n",
    "thr_used = 0.52\n",
    "final_pred = (final_prob >= thr_used).astype(int)\n",
    "\n",
    "print(f\"\\nğŸ“Š ä½¿ç”¨é˜ˆå€¼ {thr_used} çš„é¢„æµ‹ç»Ÿè®¡:\")\n",
    "print(f\"   é¢„æµ‹ä¸ºæ­£ç±»: {final_pred.sum()} ({final_pred.sum()/len(final_pred)*100:.2f}%)\")\n",
    "print(f\"   é¢„æµ‹ä¸ºè´Ÿç±»: {(1-final_pred).sum()} ({(1-final_pred).sum()/len(final_pred)*100:.2f}%)\")\n",
    "\n",
    "output_file = \"FINAL_SUBMISSION.csv\"\n",
    "pd.DataFrame({\n",
    "    \"id\": test_df.index,\n",
    "    \"label\": final_pred\n",
    "}).to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… å·²ä¿å­˜æœ€ç»ˆæäº¤æ–‡ä»¶: {output_file}\")\n",
    "print(f\"   æ ·æœ¬æ•°: {len(final_pred)}\")\n",
    "print(f\"   æ­£ç±»æ•°: {final_pred.sum()}\")\n",
    "print(f\"   è´Ÿç±»æ•°: {(1-final_pred).sum()}\")\n",
    "\n",
    "print(\"\\nğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
