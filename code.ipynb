{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è™šå‡æ–°é—»äºŒåˆ†ç±» (RTX 5090 Linux æé€Ÿç‰ˆ)\n",
    "\n",
    "**é’ˆå¯¹è‹±æ–‡æ–°é—»ä¼˜åŒ– | 32GB VRAM é€‚é… | DeBERTa-V3-Large | CLIP-ViT-Large**\n",
    "\n",
    "åŒ…å«ï¼š\n",
    "- **Part 1**: æ•°æ®æ¸…æ´—ä¸åŠ è½½\n",
    "- **Part 2**: è‹±æ–‡ TF-IDF (Word-Level) + LR åŸºçº¿\n",
    "- **Part 3**: è§†è§‰ç‰¹å¾æå– (å‡çº§ä¸º CLIP ViT-L/14 + DeiT)\n",
    "- **Part 4**: Late Fusion (æ–‡æœ¬+å›¾åƒç‰¹å¾èåˆ)\n",
    "- **Part 5**: DeBERTa-v3-large å…¨é‡å¾®è°ƒ (Bfloat16 + Compile åŠ é€Ÿ)\n",
    "- **Part 6**: æœ€ç»ˆåŠ æƒé›†æˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ è¿è¡Œç¯å¢ƒ: Linux | GPU: NVIDIA GeForce RTX 5090\n",
      "ğŸ’¾ æ˜¾å­˜æ€»é‡: 31.36 GB (èµ„æºå……è£•)\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 0: ç¯å¢ƒå‡†å¤‡ ============================\n",
    "import os, random, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# éšæœºç§å­å›ºå®š\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# RTX 5090 æ ¸å¿ƒåŠ é€Ÿè®¾ç½®\n",
    "torch.set_float32_matmul_precision('high')  # å¼€å¯ TF32\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "CACHE = \".cache\"\n",
    "os.makedirs(CACHE, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸš€ è¿è¡Œç¯å¢ƒ: Linux | GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"ğŸ’¾ æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB (èµ„æºå……è£•)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š æ•°æ®åŠ è½½ä¸­...\n",
      "============================================================\n",
      "âœ… è®­ç»ƒé›†: 7000 æ¡æ ·æœ¬\n",
      "âœ… æµ‹è¯•é›†: 3000 æ¡æ ·æœ¬\n",
      "ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ—: 'text' (å·²é€‚é…è‹±æ–‡å¤„ç†)\n",
      "\n",
      "ğŸ”„ æ–‡æœ¬æ¸…æ´—ä¸­...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8259a17414004711b8cecb2d469ae802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "è®­ç»ƒé›†æ¸…æ´—:   0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd47f5bb3bad4aad8dfb46b9c6ef119d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "æµ‹è¯•é›†æ¸…æ´—:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 1: æ•°æ®åŠ è½½ + æ–‡æœ¬é¢„å¤„ç† ============================\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š æ•°æ®åŠ è½½ä¸­...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_df = pd.read_pickle(\"train.pkl\")\n",
    "test_df  = pd.read_pickle(\"test_no_label.pkl\")\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒé›†: {train_df.shape[0]} æ¡æ ·æœ¬\")\n",
    "print(f\"âœ… æµ‹è¯•é›†: {test_df.shape[0]} æ¡æ ·æœ¬\")\n",
    "\n",
    "# è‹±æ–‡æ–‡æœ¬æ¸…æ´—å‡½æ•°\n",
    "def normalize_text(s):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    # è‹±æ–‡åªéœ€è¦å»é™¤å¤šä½™ç©ºç™½å³å¯ï¼Œä¿ç•™å¤§å°å†™æˆ–è½¬å°å†™äº¤ç»™Tokenizer\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "text_col = \"news body text\" if \"news body text\" in train_df.columns else \"text\"\n",
    "print(f\"ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ—: '{text_col}' (å·²é€‚é…è‹±æ–‡å¤„ç†)\")\n",
    "\n",
    "print(\"\\nğŸ”„ æ–‡æœ¬æ¸…æ´—ä¸­...\")\n",
    "train_df[\"text\"] = [normalize_text(t) for t in tqdm(train_df[text_col], desc=\"è®­ç»ƒé›†æ¸…æ´—\", total=len(train_df))]\n",
    "test_df[\"text\"]  = [normalize_text(t) for t in tqdm(test_df[text_col], desc=\"æµ‹è¯•é›†æ¸…æ´—\", total=len(test_df))]\n",
    "\n",
    "X = train_df[\"text\"].values\n",
    "y = train_df[\"label\"].astype(int).values\n",
    "X_test = test_df[\"text\"].values\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ å¼€å§‹ TF-IDF (è‹±æ–‡å•è¯çº§) + LR 5æŠ˜äº¤å‰éªŒè¯\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cfc2d3e16542d686eaadda1a7c70b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TF-IDF CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“¦ Fold 1: F1=0.71963 | è€—æ—¶: 4.91s\n",
      "   ğŸ“¦ Fold 2: F1=0.72218 | è€—æ—¶: 4.95s\n",
      "   ğŸ“¦ Fold 3: F1=0.70156 | è€—æ—¶: 4.98s\n",
      "   ğŸ“¦ Fold 4: F1=0.72473 | è€—æ—¶: 4.70s\n",
      "   ğŸ“¦ Fold 5: F1=0.69939 | è€—æ—¶: 4.74s\n",
      "\n",
      "ğŸ¯ TF-IDF å¹³å‡ F1: 0.71350\n",
      "âœ… æ–‡æœ¬åŸºçº¿è®­ç»ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 2: 5æŠ˜ TF-IDF (Word-Level) + LR ============================\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ å¼€å§‹ TF-IDF (è‹±æ–‡å•è¯çº§) + LR 5æŠ˜äº¤å‰éªŒè¯\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def build_tfidf_lr():\n",
    "    # å…³é”®ä¿®æ”¹ï¼šanalyzer='word', å»é™¤è‹±æ–‡åœç”¨è¯\n",
    "    vec = TfidfVectorizer(analyzer='word', stop_words='english', \n",
    "                          ngram_range=(1, 3), max_features=150000)\n",
    "    clf = LogisticRegression(solver='saga', class_weight='balanced',\n",
    "                             max_iter=500, n_jobs=-1)\n",
    "    return Pipeline([(\"tfidf\", vec), (\"lr\", clf)])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_tfidf = np.zeros(len(X))\n",
    "pred_tfidf = np.zeros(len(X_test))\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(tqdm(skf.split(X, y), total=5, desc=\"TF-IDF CV\"), 1):\n",
    "    start_time = time.time()\n",
    "    model = build_tfidf_lr()\n",
    "    \n",
    "    model.fit(X[tr], y[tr])\n",
    "    oof_tfidf[va] = model.predict_proba(X[va])[:,1]\n",
    "    pred_tfidf += model.predict_proba(X_test)[:,1] / 5\n",
    "    \n",
    "    # æŒ‡æ ‡è®¡ç®—\n",
    "    p, r, t = precision_recall_curve(y[va], oof_tfidf[va])\n",
    "    f1_scores = 2*p*r/(p+r+1e-8)\n",
    "    best_f1 = np.max(f1_scores)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   ğŸ“¦ Fold {fold}: F1={best_f1:.5f} | è€—æ—¶: {elapsed:.2f}s\")\n",
    "    fold_metrics.append(best_f1)\n",
    "\n",
    "print(f\"\\nğŸ¯ TF-IDF å¹³å‡ F1: {np.mean(fold_metrics):.5f}\")\n",
    "print(\"âœ… æ–‡æœ¬åŸºçº¿è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ–¼ï¸  å¼€å§‹å›¾åƒç‰¹å¾æå– (CLIP ViT-L-14 + DeiT)\n",
      "============================================================\n",
      "   ğŸ”„ è®­ç»ƒé›† æå– CLIP ViT-L-14 (Large) ç‰¹å¾...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28679f10ffa342d1ad6462b2d6a4f0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e70b2b113c4ee9af9cafcf60e1bd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   CLIP è®­ç»ƒé›†:   0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… ä¿å­˜åˆ°: .cache/clip_large_train.npz (Shape: (7000, 768))\n",
      "   ğŸ”„ æµ‹è¯•é›† æå– CLIP ViT-L-14 (Large) ç‰¹å¾...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aae5e420c22446da5fd2f9d3b11398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   CLIP æµ‹è¯•é›†:   0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âœ… ä¿å­˜åˆ°: .cache/clip_large_test.npz (Shape: (3000, 768))\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/deit_train.npz\n",
      "   âœ… åŠ è½½ç¼“å­˜: .cache/deit_test.npz\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆå›¾åƒç‰¹å¾ç»´åº¦: (7000, 1536)\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 3: å›¾åƒç‰¹å¾æå– (ViT-L-14 å‡çº§ç‰ˆ) ============================\n",
    "import cv2, torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import timm\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¼ï¸  å¼€å§‹å›¾åƒç‰¹å¾æå– (CLIP ViT-L-14 + DeiT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def extract_clip(paths, save_path, dataset_name=\"\"):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"   âœ… åŠ è½½ç¼“å­˜: {save_path}\")\n",
    "        return np.load(save_path)['feats']\n",
    "    \n",
    "    # å‡çº§ï¼šä½¿ç”¨ ViT-L-14 (Large) æ¨¡å‹ï¼Œopenai é¢„è®­ç»ƒæƒé‡\n",
    "    print(f\"   ğŸ”„ {dataset_name} æå– CLIP ViT-L-14 (Large) ç‰¹å¾...\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "    model.to(device).eval()\n",
    "    \n",
    "    feats = []\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "        for p in tqdm(paths, desc=f\"   CLIP {dataset_name}\", leave=False):\n",
    "            try:\n",
    "                img = Image.open(os.path.join(\"image\", p)).convert(\"RGB\")\n",
    "                x = preprocess(img).unsqueeze(0).to(device)\n",
    "                f = model.encode_image(x)\n",
    "                f = f / f.norm(dim=-1, keepdim=True) # å½’ä¸€åŒ–\n",
    "                feats.append(f.float().cpu().numpy()[0])\n",
    "            except:\n",
    "                feats.append(np.zeros(768)) # ViT-L ç»´åº¦æ˜¯ 768\n",
    "    \n",
    "    feats = np.vstack(feats)\n",
    "    np.savez_compressed(save_path, feats=feats)\n",
    "    print(f\"      âœ… ä¿å­˜åˆ°: {save_path} (Shape: {feats.shape})\")\n",
    "    return feats\n",
    "\n",
    "def extract_deit(paths, save_path, dataset_name=\"\"):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"   âœ… åŠ è½½ç¼“å­˜: {save_path}\")\n",
    "        return np.load(save_path)['feats']\n",
    "    \n",
    "    print(f\"   ğŸ”„ {dataset_name} æå– DeiT ç‰¹å¾...\")\n",
    "    model = timm.create_model(\"deit_base_patch16_224.fb_in1k\", pretrained=True, num_classes=0).to(device).eval()\n",
    "    trans = timm.data.create_transform(**timm.data.resolve_model_data_config(model))\n",
    "    \n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for p in tqdm(paths, desc=f\"   DeiT {dataset_name}\", leave=False):\n",
    "            try:\n",
    "                img = Image.open(os.path.join(\"image\", p)).convert(\"RGB\")\n",
    "                x = trans(img).unsqueeze(0).to(device)\n",
    "                feats.append(model(x).cpu().numpy()[0])\n",
    "            except:\n",
    "                feats.append(np.zeros(768))\n",
    "                \n",
    "    feats = np.vstack(feats)\n",
    "    np.savez_compressed(save_path, feats=feats)\n",
    "    return feats\n",
    "\n",
    "# æå–å¹¶èåˆ\n",
    "clip_train = extract_clip(train_df['image_path'], f\"{CACHE}/clip_large_train.npz\", \"è®­ç»ƒé›†\")\n",
    "clip_test  = extract_clip(test_df['image_path'],  f\"{CACHE}/clip_large_test.npz\", \"æµ‹è¯•é›†\")\n",
    "deit_train = extract_deit(train_df['image_path'], f\"{CACHE}/deit_train.npz\", \"è®­ç»ƒé›†\")\n",
    "deit_test  = extract_deit(test_df['image_path'],  f\"{CACHE}/deit_test.npz\", \"æµ‹è¯•é›†\")\n",
    "\n",
    "img_train = np.hstack([clip_train, deit_train])\n",
    "img_test  = np.hstack([clip_test,  deit_test])\n",
    "print(f\"\\nğŸ“Š æœ€ç»ˆå›¾åƒç‰¹å¾ç»´åº¦: {img_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ”— å¼€å§‹ Late Fusion (TF-IDF + ViT-L + DeiT)\n",
      "============================================================\n",
      "   âœ… èåˆç‰¹å¾ç»´åº¦: (7000, 151536)\n",
      "ğŸ“Š èåˆæ¨¡å‹è®­ç»ƒé›†æœ€ä½³ F1: 0.90509\n",
      "âœ… Late Fusion å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 4: æ™šèåˆ (OOF ä¿®å¤ç‰ˆ) ============================\n",
    "from scipy import sparse\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ”— å¼€å§‹ Late Fusion (5-Fold OOF CV) - å·²ä¿®å¤è¿‡æ‹Ÿåˆé—®é¢˜\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vec = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 3), max_features=150000)\n",
    "text_train = vec.fit_transform(X)\n",
    "text_test  = vec.transform(X_test)\n",
    "\n",
    "Xfull = sparse.hstack([text_train, sparse.csr_matrix(img_train)])\n",
    "Xtest_full = sparse.hstack([text_test,  sparse.csr_matrix(img_test)])\n",
    "\n",
    "print(f\"   âœ… èåˆç‰¹å¾ç»´åº¦: {Xfull.shape}\")\n",
    "\n",
    "# ä¿®å¤é‡ç‚¹ï¼šä½¿ç”¨ StratifiedKFold ç”Ÿæˆ OOF é¢„æµ‹ï¼Œè€Œä¸æ˜¯å…¨é‡è®­ç»ƒå…¨é‡é¢„æµ‹\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "oof_fusion = np.zeros(len(X))\n",
    "prob_fusion = np.zeros(len(X_test))\n",
    "clf_fusion = LogisticRegression(solver='saga', class_weight='balanced', max_iter=1000, n_jobs=-1)\n",
    "fold_metrics = []\n",
    "\n",
    "print(\"   ğŸ”„ æ­£åœ¨è¿›è¡Œäº¤å‰éªŒè¯...\")\n",
    "for fold, (tr, va) in enumerate(tqdm(skf.split(Xfull, y), total=5, desc=\"Fusion CV\"), 1):\n",
    "    X_tr, X_va = Xfull.tocsr()[tr], Xfull.tocsr()[va]\n",
    "    y_tr, y_va = y[tr], y[va]\n",
    "    \n",
    "    clf_fusion.fit(X_tr, y_tr)\n",
    "    oof_fusion[va] = clf_fusion.predict_proba(X_va)[:,1]\n",
    "    prob_fusion += clf_fusion.predict_proba(Xtest_full)[:,1] / 5\n",
    "    \n",
    "    p, r, t = precision_recall_curve(y_va, oof_fusion[va])\n",
    "    f1_fold = np.max(2*p*r/(p+r+1e-8))\n",
    "    fold_metrics.append(f1_fold)\n",
    "\n",
    "print(f\"ğŸ“Š èåˆæ¨¡å‹çœŸå® CV F1: {np.mean(fold_metrics):.5f} (ä¸å†æ˜¯è™šé«˜çš„ 0.90!)\")\n",
    "print(\"âœ… Late Fusion ä¿®å¤å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ¤– å¼€å§‹ DeBERTa-v3-large 5æŠ˜äº¤å‰éªŒè¯ (Linux 5090 åŠ é€Ÿç‰ˆ)\n",
      "============================================================\n",
      "ğŸ“¥ åŠ è½½æ¨¡å‹: microsoft/deberta-v3-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0d787c0a7d4945b6723158ac3a897e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4725b77b74564792aaaa609044b9e440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3d7056db-358a-4e93-af42-77c9d5147023)')' thrown while requesting GET https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc136468d709f17dee5/ceef70ee5c068f2e8ec2ea787d1566d2f5846b612f0e0a3879edffa246dc91cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251210T060221Z&X-Amz-Expires=3600&X-Amz-Signature=3839e117644e3da2ee35967a35264f3f65cbf5050a098c2d18bd1cc56aaab8f0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=62171e3b6a99db28e0b3159d&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27spm.model%3B+filename%3D%22spm.model%22%3B&x-id=GetObject&Expires=1765350141&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTM1MDE0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxN2RlZTUvY2VlZjcwZWU1YzA2OGYyZThlYzJlYTc4N2QxNTY2ZDJmNTg0NmI2MTJmMGUwYTM4NzllZGZmYTI0NmRjOTFjZioifV19&Signature=Xjnxz84uZaaFvOZj2hC%7EDbOfB-yWu2KRr1j3gPfOY1jOmOh84wmibZGFwx9ZU8vw8%7EbpUGl22ZCwxdIOt1jh6s-2FU8DDSXaXBCnZdQYv7On8%7EtcVehKyOKzuj6JETn42Z9DedInN48EGVHp-DYdPXioGHze3mmQHy2OKWM6EiL4jSNUCDOf-s3lgm7b-WNacr-9ea8lrNKpFpSkFgKje5Igle89SItnUAjc3SCl9ow745E7g32DTdJnSJP2oHz7xJEtbMIYvq0go0ShH2aUQbpT78rGaWLOERZhcOjMhqTEAXEqLB1HmowsZCehsOePgRzhkbrL2iRJINza2wPAlQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3d7056db-358a-4e93-af42-77c9d5147023)')' thrown while requesting GET https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc136468d709f17dee5/ceef70ee5c068f2e8ec2ea787d1566d2f5846b612f0e0a3879edffa246dc91cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251210T060221Z&X-Amz-Expires=3600&X-Amz-Signature=3839e117644e3da2ee35967a35264f3f65cbf5050a098c2d18bd1cc56aaab8f0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=62171e3b6a99db28e0b3159d&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27spm.model%3B+filename%3D%22spm.model%22%3B&x-id=GetObject&Expires=1765350141&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTM1MDE0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxN2RlZTUvY2VlZjcwZWU1YzA2OGYyZThlYzJlYTc4N2QxNTY2ZDJmNTg0NmI2MTJmMGUwYTM4NzllZGZmYTI0NmRjOTFjZioifV19&Signature=Xjnxz84uZaaFvOZj2hC%7EDbOfB-yWu2KRr1j3gPfOY1jOmOh84wmibZGFwx9ZU8vw8%7EbpUGl22ZCwxdIOt1jh6s-2FU8DDSXaXBCnZdQYv7On8%7EtcVehKyOKzuj6JETn42Z9DedInN48EGVHp-DYdPXioGHze3mmQHy2OKWM6EiL4jSNUCDOf-s3lgm7b-WNacr-9ea8lrNKpFpSkFgKje5Igle89SItnUAjc3SCl9ow745E7g32DTdJnSJP2oHz7xJEtbMIYvq0go0ShH2aUQbpT78rGaWLOERZhcOjMhqTEAXEqLB1HmowsZCehsOePgRzhkbrL2iRJINza2wPAlQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a4cb21fdb4409fab6df58bb0c4587b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Fold 1/5 å¼€å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 14:02:37.651390: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 14:02:37.693384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 14:02:38.411945: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dae7bbb32f44e7baab92a1163091906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Torch Compile å·²å¯ç”¨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2551531db8444b01bf4956b4fc361a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 1/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006b6aa20be34e8f870e08969edcc03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 1 Val F1: 0.77905 (Loss: 0.5190)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b703f6eb8f4533a0ab6eb52a4119d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 2/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 2 Val F1: 0.79156 (Loss: 0.3916)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87361be6e141379b3d76e11b45e123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 3/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 3 Val F1: 0.79599 (Loss: 0.2768)\n",
      "   ğŸ‰ Fold 1 æœ€ä½³ F1: 0.79599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e20c00ff27f4044a75b4b6ab8b65d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Test Pred:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Fold 2/5 å¼€å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Torch Compile å·²å¯ç”¨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488b460df15e4340b4119a4c8ea51ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 1/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 1 Val F1: 0.75898 (Loss: 0.5234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdca87869ea43e697ccb739ec8f04a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 2/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 2 Val F1: 0.77476 (Loss: 0.3997)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6877f00516ee436fa6afb1d5478ffacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 3/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 3 Val F1: 0.77768 (Loss: 0.2927)\n",
      "   ğŸ‰ Fold 2 æœ€ä½³ F1: 0.77768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5ed9619fb64d75a0199052da02aa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Test Pred:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Fold 3/5 å¼€å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Torch Compile å·²å¯ç”¨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea191bb99dfc4372854d0ae5f0585276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 1/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 1 Val F1: 0.76108 (Loss: 0.5156)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fe2a20971f4d0ba0bf6c5c7907784a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 2/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 2 Val F1: 0.77129 (Loss: 0.3777)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf45f21c2cb40c0937594ab4b057586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 3/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 3 Val F1: 0.76748 (Loss: 0.2465)\n",
      "   ğŸ‰ Fold 3 æœ€ä½³ F1: 0.77129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae26cb8b001944698618ca3ff10be627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Test Pred:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Fold 4/5 å¼€å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Torch Compile å·²å¯ç”¨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2af99ae4fb4931aa48ed49d94873b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 1/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 1 Val F1: 0.77504 (Loss: 0.5115)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40916b1f9e294e6b8f04f7145e799fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 2/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 2 Val F1: 0.79292 (Loss: 0.3850)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b430f760c794e3fbcb624d967c6658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 3/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 3 Val F1: 0.79866 (Loss: 0.2582)\n",
      "   ğŸ‰ Fold 4 æœ€ä½³ F1: 0.79866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129cb4ac25644a1c85f566ae611335b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Test Pred:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Fold 5/5 å¼€å§‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš¡ Torch Compile å·²å¯ç”¨\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b12745a6c64e0bbe7df061649f5f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 1/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 1 Val F1: 0.74395 (Loss: 0.5078)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c51f11f0fa47fdae007e830e3a6af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 2/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 2 Val F1: 0.76391 (Loss: 0.3800)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2050037b5774585a19a5d0836d59a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Epoch 3/3:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Epoch 3 Val F1: 0.76850 (Loss: 0.2616)\n",
      "   ğŸ‰ Fold 5 æœ€ä½³ F1: 0.76850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83186bd0bea4609b255dd657d7904b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   Test Pred:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† DeBERTa CV å¹³å‡ F1: 0.78243\n",
      "âœ… DeBERTa è®­ç»ƒå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 5: DeBERTa-v3-large å¼ºåŠ›å¾®è°ƒ ============================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¤– å¼€å§‹ DeBERTa-v3-large 5æŠ˜äº¤å‰éªŒè¯ (Linux 5090 åŠ é€Ÿç‰ˆ)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# é…ç½®\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\" # è‹±æ–‡ SOTA æ¨¡å‹\n",
    "BATCH_SIZE = 16       # 5090 æ˜¾å­˜å¤§ï¼Œç›´æ¥ä¸Šå¤§ Batch\n",
    "ACCUM_STEPS = 1       \n",
    "EPOCHS = 3            # DeBERTa æ”¶æ•›å¿«\n",
    "LR = 8e-6             # Large æ¨¡å‹å­¦ä¹ ç‡è¦å°\n",
    "MAX_LEN = 512\n",
    "\n",
    "print(f\"ğŸ“¥ åŠ è½½æ¨¡å‹: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        encoding = tokenizer(str(self.texts[i]), truncation=True, padding='max_length', \n",
    "                             max_length=MAX_LEN, return_tensors='pt')\n",
    "        item = {k: v.flatten() for k,v in encoding.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=SEED)\n",
    "oof_deberta = np.zeros(len(X))\n",
    "pred_deberta = np.zeros(len(X_test))\n",
    "fold_metrics = []\n",
    "\n",
    "test_ds = NewsDataset(X_test)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=False, \n",
    "                         num_workers=8, pin_memory=True)\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nğŸ“¦ Fold {fold}/5 å¼€å§‹...\")\n",
    "    \n",
    "    # æ•°æ®åŠ è½½ (Linux å¤šè¿›ç¨‹ä¼˜åŒ–)\n",
    "    train_ds = NewsDataset(X[tr], y[tr])\n",
    "    val_ds = NewsDataset(X[va], y[va])\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=8, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, \n",
    "                            num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # æ¨¡å‹åˆå§‹åŒ– & ç¼–è¯‘\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Linux ä¸‹å¯ç”¨ torch.compile åŠ é€Ÿ (5090 æ”¯æŒæä½³)\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"   âš¡ Torch Compile å·²å¯ç”¨\")\n",
    "    except: pass\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100, \n",
    "                                                num_training_steps=len(train_loader)*EPOCHS)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(train_loader, desc=f\"   Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        \n",
    "        for batch in pbar:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            # Bfloat16 æ··åˆç²¾åº¦ (æ— éœ€ Scalerï¼Œæ›´å¿«æ›´ç¨³)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        # éªŒè¯\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    outputs = model(**inputs)\n",
    "                preds.extend(torch.softmax(outputs.logits, dim=1)[:,1].float().cpu().numpy())\n",
    "                trues.extend(inputs['labels'].cpu().numpy())\n",
    "        \n",
    "        p, r, t = precision_recall_curve(trues, preds)\n",
    "        f1 = np.max(2*p*r/(p+r+1e-8))\n",
    "        print(f\"      Epoch {epoch+1} Val F1: {f1:.5f} (Loss: {np.mean(losses):.4f})\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            oof_deberta[va] = np.array(preds)\n",
    "            # ä¿å­˜ç¼–è¯‘åçš„æ¨¡å‹æƒé‡æœ‰ç‚¹éº»çƒ¦ï¼Œè¿™é‡Œåªä¿å­˜æœ€ä½³çŠ¶æ€å­—å…¸çš„å‰¯æœ¬\n",
    "            # è¿™é‡Œçš„trickæ˜¯é‡æ–°è·å– uncompiled model çš„ state dict\n",
    "            if hasattr(model, '_orig_mod'):\n",
    "                best_weights = {k: v.cpu() for k, v in model._orig_mod.state_dict().items()}\n",
    "            else:\n",
    "                best_weights = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"   ğŸ‰ Fold {fold} æœ€ä½³ F1: {best_f1:.5f}\")\n",
    "    fold_metrics.append(best_f1)\n",
    "    \n",
    "    # é¢„æµ‹æµ‹è¯•é›†\n",
    "    if hasattr(model, '_orig_mod'):\n",
    "        model._orig_mod.load_state_dict(best_weights)\n",
    "    else:\n",
    "        model.load_state_dict(best_weights)\n",
    "    \n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"   Test Pred\", leave=False):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                outputs = model(**inputs)\n",
    "            fold_preds.extend(torch.softmax(outputs.logits, dim=1)[:,1].float().cpu().numpy())\n",
    "    pred_deberta += np.array(fold_preds) / 5\n",
    "    \n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nğŸ† DeBERTa CV å¹³å‡ F1: {np.mean(fold_metrics):.5f}\")\n",
    "print(\"âœ… DeBERTa è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âš–ï¸  æœ€ç»ˆåŠ æƒèåˆ (DeBERTa + Late Fusion)\n",
      "============================================================\n",
      "ğŸ¯ æœ€ä½³æƒé‡é…ç½®:\n",
      "   DeBERTa-v3-large: 0.00\n",
      "   Late Fusion (ViT+TFIDF): 1.00\n",
      "   ğŸ† CV æœ€ä½³ F1: 0.90509\n",
      "ğŸšª æœ€ä½³åˆ†ç±»é˜ˆå€¼: 0.4716\n",
      "\n",
      "âœ… ç»“æœå·²ä¿å­˜è‡³: FINAL_SUBMISSION_5090.csv\n",
      "   æ­£ä¾‹æ¯”ä¾‹: 44.43%\n",
      "ğŸ‰ å…¨éƒ¨æµç¨‹ç»“æŸï¼\n"
     ]
    }
   ],
   "source": [
    "# ============================ PART 6: æœ€ç»ˆèåˆ (Ensemble) ============================\n",
    "print(\"=\"*60)\n",
    "print(\"âš–ï¸  æœ€ç»ˆåŠ æƒèåˆ (DeBERTa + Late Fusion)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è‡ªåŠ¨æœç´¢æœ€ä½³æƒé‡\n",
    "# æ³¨æ„ï¼šç°åœ¨æˆ‘ä»¬ä½¿ç”¨çœŸå®çš„ oof_fusionï¼Œè€Œä¸æ˜¯è™šé«˜çš„ train_prob_fusion\n",
    "best_w = 0\n",
    "best_score = 0\n",
    "for w in np.arange(0.0, 1.01, 0.01):\n",
    "    # w æ˜¯ DeBERTa çš„æƒé‡\n",
    "    blend = w * oof_deberta + (1-w) * oof_fusion\n",
    "    p, r, t = precision_recall_curve(y, blend)\n",
    "    f1 = np.max(2*p*r/(p+r+1e-8))\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_w = w\n",
    "\n",
    "print(f\"ğŸ¯ æœ€ä½³æƒé‡é…ç½®:\")\n",
    "print(f\"   DeBERTa-v3-large: {best_w:.2f}\")\n",
    "print(f\"   Late Fusion (ViT+TFIDF): {1-best_w:.2f}\")\n",
    "print(f\"   ğŸ† CV æœ€ä½³ F1: {best_score:.5f}\")\n",
    "\n",
    "# ç”Ÿæˆæœ€ç»ˆé¢„æµ‹\n",
    "final_prob = best_w * pred_deberta + (1-best_w) * prob_fusion\n",
    "\n",
    "# å¯»æ‰¾æœ€ä½³é˜ˆå€¼\n",
    "blend_oof = best_w * oof_deberta + (1-best_w) * oof_fusion\n",
    "p, r, t = precision_recall_curve(y, blend_oof)\n",
    "f1s = 2*p*r/(p+r+1e-8)\n",
    "best_threshold = t[np.argmax(f1s)]\n",
    "\n",
    "print(f\"ğŸšª æœ€ä½³åˆ†ç±»é˜ˆå€¼: {best_threshold:.4f}\")\n",
    "final_preds_binary = (final_prob >= best_threshold).astype(int)\n",
    "\n",
    "output_file = \"FINAL_SUBMISSION_5090.csv\"\n",
    "pd.DataFrame({\n",
    "    \"id\": test_df.index,\n",
    "    \"label\": final_preds_binary\n",
    "}).to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_file}\")\n",
    "print(f\"   æ­£ä¾‹æ¯”ä¾‹: {final_preds_binary.mean():.2%}\")\n",
    "print(\"ğŸ‰ å…¨éƒ¨æµç¨‹ç»“æŸï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
